{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_dDfmN6gRvg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from patchify import patchify\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FDA4OL7gg1Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf /content/drive/MyDrive/flower_photos.tgz"
      ],
      "metadata": {
        "id": "oYJPMLZFgyjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ],
      "metadata": {
        "id": "AS6BLBlCiHe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Hyperparameters \"\"\"\n",
        "hp = {}\n",
        "hp[\"image_size\"] = 200\n",
        "hp[\"num_channels\"] = 3\n",
        "hp[\"patch_size\"] = 25\n",
        "hp[\"num_patches\"] = (hp[\"image_size\"] **2) // (hp[\"patch_size\"] **2)\n",
        "hp[\"flat_patches_shape\"] = (hp[\"num_patches\"], hp[\"patch_size\"] * hp[\"patch_size\"] * hp[\"num_channels\"])\n",
        "\n",
        "hp[\"batch_size\"] = 8\n",
        "hp[\"learning_rate\"] = 1e-4\n",
        "hp[\"num_epochs\"] = 500\n",
        "hp[\"num_classes\"] = 5\n",
        "hp[\"class_names\"] = [\"daisy\", \"dandelion\", \"roses\", \"sunflowers\", \"tulips\"]\n",
        "\n",
        "hp[\"num_layers\"] = 12\n",
        "hp[\"hidden_dim\"] = 768\n",
        "hp[\"mlp_dim\"] = 3072\n",
        "hp[\"num_heads\"] = 12\n",
        "hp[\"dropout_rate\"] = 0.1"
      ],
      "metadata": {
        "id": "nRtbNH9FiEzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/flower_photos\""
      ],
      "metadata": {
        "id": "cXbPGDvhiHeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(dataset_path, split=0.1):\n",
        "    images = shuffle(glob(os.path.join(dataset_path, \"*\", \"*.jpg\")))\n",
        "    #print(len(images))\n",
        "\n",
        "    split_size = int(len(images) * split)\n",
        "\n",
        "    X_train, X_val = train_test_split(images, test_size=split_size, random_state=42)\n",
        "    X_train, X_test = train_test_split(X_train, test_size=split_size, random_state=42)\n",
        "\n",
        "    return X_train, X_val, X_test"
      ],
      "metadata": {
        "id": "xRZ5FJQCiUeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image_label(path):\n",
        "    #print(path)\n",
        "    path = path.decode()\n",
        "    #print(path)\n",
        "    image = cv2.imread(path, cv2.IMREAD_COLOR)\n",
        "    image = cv2.resize(image, (hp[\"image_size\"], hp[\"image_size\"]))\n",
        "    image = image / 255.0\n",
        "    #print(image.shape)\n",
        "\n",
        "    \"\"\" Preprocessing to patches \"\"\"\n",
        "    patch_shape = (hp[\"patch_size\"], hp[\"patch_size\"], hp[\"num_channels\"])\n",
        "    patches = patchify(image, patch_shape, hp[\"patch_size\"])\n",
        "    # patches = np.reshape(patches, (64, 25, 25, 3))\n",
        "    # for i in range(64):\n",
        "    #     cv2.imwrite(save_path + f\"\\\\files\\\\{i}.png\", patches[i])\n",
        "\n",
        "    patches = np.reshape(patches, hp[\"flat_patches_shape\"])\n",
        "    patches = patches.astype(np.float32)\n",
        "\n",
        "    \"\"\" Label \"\"\"\n",
        "\n",
        "    class_name = path.split(\"/\")[-2]\n",
        "    #print(class_name)\n",
        "    class_index = hp[\"class_names\"].index(class_name)\n",
        "    #print(class_index)\n",
        "    class_index = np.array(class_index, dtype=np.int32)\n",
        "    #print(patches.shape)\n",
        "\n",
        "    return patches, class_index"
      ],
      "metadata": {
        "id": "7cZvscGGiUen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse(path):\n",
        "    patches, labels = tf.numpy_function(process_image_label, [path], [tf.float32, tf.int32])\n",
        "    labels = tf.one_hot(labels, hp[\"num_classes\"])\n",
        "\n",
        "    patches.set_shape(hp[\"flat_patches_shape\"])\n",
        "    labels.set_shape(hp[\"num_classes\"])\n",
        "\n",
        "    return patches, labels"
      ],
      "metadata": {
        "id": "0XlZupeKiUkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_dataset(images, batch=32):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((images))\n",
        "    ds = ds.map(parse).batch(batch).prefetch(8)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "0aLUK61lic4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassToken(Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(\n",
        "            initial_value = w_init(shape=(1, 1, input_shape[-1]), dtype=tf.float32),\n",
        "            trainable = True\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        hidden_dim = self.w.shape[-1]\n",
        "\n",
        "        cls = tf.broadcast_to(self.w, [batch_size, 1, hidden_dim])\n",
        "        cls = tf.cast(cls, dtype=inputs.dtype)\n",
        "        return cls"
      ],
      "metadata": {
        "id": "-ktFdIX0gyrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, cf):\n",
        "    x = Dense(cf[\"mlp_dim\"], activation=\"gelu\")(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    x = Dense(cf[\"hidden_dim\"])(x)\n",
        "    x = Dropout(cf[\"dropout_rate\"])(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "UHhez8OjhuNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer_encoder(x, cf):\n",
        "    skip_1 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = MultiHeadAttention(\n",
        "        num_heads=cf[\"num_heads\"], key_dim=cf[\"hidden_dim\"]\n",
        "    )(x, x)\n",
        "    x = Add()([x, skip_1])\n",
        "\n",
        "    skip_2 = x\n",
        "    x = LayerNormalization()(x)\n",
        "    x = mlp(x, cf)\n",
        "    x = Add()([x, skip_2])\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "YrBZvoq1huPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ViT(cf):\n",
        "    input_shape = (cf[\"num_patches\"], cf[\"patch_size\"] * cf[\"patch_size\"] * cf[\"num_channels\"])\n",
        "    inputs = Input(input_shape)\n",
        "    #print(inputs.shape)\n",
        "\n",
        "    \"\"\" Patch + Position Embeddings \"\"\"\n",
        "    patch_embed = Dense(cf[\"hidden_dim\"])(inputs)\n",
        "    #print(patch_embed.shape)\n",
        "\n",
        "    positions = tf.range(start=0, limit=cf[\"num_patches\"], delta=1)\n",
        "    #print(positions)\n",
        "    pos_embed = Embedding(input_dim=cf[\"num_patches\"], output_dim=cf[\"hidden_dim\"])(positions)\n",
        "    #print(pos_embed)\n",
        "\n",
        "    embed = patch_embed + pos_embed\n",
        "    #print(embed.shape)\n",
        "\n",
        "    \"\"\" Adding Class Tokec \"\"\"\n",
        "    token = ClassToken()(embed)\n",
        "    x = Concatenate(axis=1)([token, embed])\n",
        "    #print(x.shape)\n",
        "\n",
        "    for _ in range(cf[\"num_layers\"]):\n",
        "        x = transformer_encoder(x, cf)\n",
        "\n",
        "    #print(x.shape)\n",
        "\n",
        "    \"\"\" Classification Head \"\"\"\n",
        "    x = LayerNormalization()(x)\n",
        "    x = x[:, 0, :]\n",
        "    #print(x.shape)\n",
        "    x = Dense(cf[\"num_classes\"], activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs, x)\n",
        "    return model"
      ],
      "metadata": {
        "id": "9OAVZyjIhuR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    config = {}\n",
        "    config[\"num_layers\"] = 12\n",
        "    config[\"hidden_dim\"] = 768\n",
        "    config[\"mlp_dim\"] = 3072\n",
        "    config[\"num_heads\"] = 12\n",
        "    config[\"dropout_rate\"] = 0.1\n",
        "    config[\"num_patches\"] = 256\n",
        "    config[\"patch_size\"] = 32\n",
        "    config[\"num_channels\"] = 3\n",
        "    config[\"num_classes\"] = 5\n",
        "\n",
        "    model = ViT(config)\n",
        "    model.summary()"
      ],
      "metadata": {
        "id": "ow0ZIrrihuSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Directory for storing files \"\"\"\n",
        "    create_dir(\"/content/drive/MyDrive/files\" + \"/flowers\")\n",
        "\n",
        "    \"\"\" Paths \"\"\"\n",
        "    model_path = os.path.join(\"/content/drive/MyDrive/files\", \"flowers\", \"Image_Classification_using_Vision_Transformer_Vit.h5\")\n",
        "    csv_path = os.path.join(\"/content/drive/MyDrive/files\", \"flowers\", \"Image_Classification_using_Vision_Transformer_Vit.csv \")\n",
        "\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    X_train, X_val, X_test = load_data(dataset_path)\n",
        "    #print(f\"Train - {len(X_train)}, Validation - {len(X_val)}, Test - {len(X_test)}\")\n",
        "\n",
        "    #process_image_label(X_train[0], dataset_path)\n",
        "\n",
        "    train_dataset = tf_dataset(X_train, batch=hp[\"batch_size\"])\n",
        "    val_dataset = tf_dataset(X_val, batch=hp[\"batch_size\"])\n",
        "    # for x, y in train_dataset:\n",
        "    #     print(x.shape, y.shape)\n",
        "    #     break\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model = ViT(hp)\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp[\"learning_rate\"], clipvalue=1.0),\n",
        "        metrics = [\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(model_path, monitor=\"val_loss\", verbose=1, save_best_only=True),\n",
        "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=10, min_lr=1e-1),\n",
        "        CSVLogger(csv_path),\n",
        "        EarlyStopping(monitor=\"val_loss\", patience=50, restore_best_weights=False)\n",
        "    ]"
      ],
      "metadata": {
        "id": "GWr8jg-khuU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    train_dataset,\n",
        "    epochs=hp[\"num_epochs\"],\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "eVPlIScFhuXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    \"\"\" Seeding \"\"\"\n",
        "    np.random.seed(42)\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    \"\"\" Paths \"\"\"\n",
        "    model_path = os.path.join(\"/content/drive/MyDrive/files\", \"flowers\", \"Image_Classification_using_Vision_Transformer_Vit.h5\")\n",
        "\n",
        "    \"\"\" Dataset \"\"\"\n",
        "    X_train, X_val, X_test = load_data(dataset_path)\n",
        "    print(f\"Train - {len(X_train)}, Validation - {len(X_val)}, Test - {len(X_test)}\")\n",
        "\n",
        "    test_ds = tf_dataset(X_test, batch=hp[\"batch_size\"])\n",
        "\n",
        "    \"\"\" Model \"\"\"\n",
        "    model = ViT(hp)\n",
        "    model.load_weights(model_path)\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp[\"learning_rate\"], clipvalue=1.0),\n",
        "        metrics = [\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "alEFA-zNhuZ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}